# Emotion-Detection-in-Audio
  Emotion Recognition in Audio:  Emotion Recognition from Non-Speech Audio This project classifies emotions based on non-speech audio signals using deep learning models.  
  Models Used: 
  CNN: Captures spatial patterns in audio features like MFCCs to identify frequency modulations. 
  LSTM: Models temporal dependencies to track changes in the audio over time. 
  CNN+LSTM Hybrid: Combines spatial and temporal features for enhanced emotion recognition. Pre-trained models were used for improved accuracy, and future work will focus on optimizing feature extraction and model fine-tuning.
